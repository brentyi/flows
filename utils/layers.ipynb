{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializers\n",
    "\n",
    "def actnorm_layer(tf_in, op_init_actnorm, name=\"\", reverse=False):    \n",
    "    ones_init = 5\n",
    "    zeros_init = tf.zeros_initializer()\n",
    "    ones_init = tf.ones_initializer()\n",
    "\n",
    "    with tf.variable_scope(name+'act_norm', reuse=tf.AUTO_REUSE):\n",
    "        # Actual actnorm\n",
    "        #print(tf_in)\n",
    "        channels = tf_in.shape[-1]\n",
    "        tf_log_scale = tf.get_variable(\"scale\", shape=(channels), initializer=ones_init)\n",
    "        tf_scale = tf.exp(tf_log_scale)\n",
    "        tf_bias  = tf.get_variable(\"bias\" , shape=(channels), initializer=zeros_init)\n",
    "        \n",
    "        if not reverse:  \n",
    "            tf_out = tf_in\n",
    "            tf_out *= tf_scale\n",
    "            tf_out += tf_bias\n",
    "\n",
    "            # Actnorm initialization\n",
    "            tf_mean, tf_variance = tf.nn.moments(tf_in, axes=[0, 1, 2])\n",
    "            tf_new_scale = tf_scale / tf.math.sqrt(tf_variance)\n",
    "            tf_new_bias = tf_bias - tf_mean * tf_new_scale\n",
    "            op_init_actnorm.append([\n",
    "                tf_log_scale.assign(tf.log(tf_new_scale)),\n",
    "                tf_bias.assign(tf_new_bias)\n",
    "            ])\n",
    "\n",
    "            tf_log_jacobian_determinant = tf_in.shape[1].value * tf_in.shape[2].value * tf.reduce_sum(tf_log_scale)\n",
    "            return tf_out, tf_log_jacobian_determinant\n",
    "        \n",
    "        else:\n",
    "            tf_inverse_out = tf_in\n",
    "            tf_inverse_out = tf_inverse_out - tf_bias\n",
    "            tf_inverse_out = tf_inverse_out / tf_scale\n",
    "\n",
    "            return tf_inverse_out\n",
    "        \n",
    "def glow_coupling_layer(tf_in, op_init_actnorm=None, reverse=False, return_conv_features=False):\n",
    "    CHANNEL_WIDTH = 512\n",
    "    \n",
    "    with tf.variable_scope(\"glow_module\", reuse=tf.AUTO_REUSE):\n",
    "        if not reverse: \n",
    "            tf_in, log_det1 = actnorm_layer(tf_in, op_init_actnorm)\n",
    "            tf_in, log_det2 = invertible_1x1_conv(tf_in)\n",
    "            tf_in, log_det3, conv_features = coupling_layer(tf_in, CHANNEL_WIDTH)\n",
    "            #print(log_det1.get_shape(), log_det2.get_shape(), log_det3.get_shape())\n",
    "            if return_conv_features:\n",
    "                return tf_in, log_det1 + log_det2 + log_det3, conv_features\n",
    "            else:\n",
    "                return tf_in, log_det1 + log_det2 + log_det3\n",
    "            #return tf_in,  log_det2 + log_det3\n",
    "        else:\n",
    "            tf_in = coupling_layer(tf_in, CHANNEL_WIDTH, reverse=True)\n",
    "            tf_in = invertible_1x1_conv(tf_in, reverse=True)\n",
    "            tf_in = actnorm_layer(tf_in, op_init_actnorm, reverse=True)\n",
    "            return tf_in\n",
    "\n",
    "def squeeze_layer(tf_in, reverse=False):\n",
    "    # [b, h, w, c] => [b, h // 2, w // 2, c * 4]\n",
    "    if not reverse:\n",
    "        tf_out = tf.nn.space_to_depth(tf_in, 2)\n",
    "    else:\n",
    "        tf_out = tf.nn.depth_to_space(tf_in, 2)\n",
    "    return tf_out\n",
    "        \n",
    "def preprocess_layer(tf_in, reverse=False):\n",
    "    alpha = 0.05\n",
    "    max_pixel_value = 256\n",
    "    shape = int_shape(tf_in)\n",
    "    DIM = shape[1] * shape[2] * shape[3]\n",
    "    # initial scaling\n",
    "    # (1.0 - 2 * alpha) makes more sense than\n",
    "    # (1 - alpha) to me... TODO check???\n",
    "    \n",
    "    if not reverse:        \n",
    "        # normalization\n",
    "        tf_x = (alpha + (1.0 - 2.0*alpha) * (tf_in) / max_pixel_value)\n",
    "        # compute logit function\n",
    "        tf_out = tf.log(tf_x / (1 - tf_x) + 1e-10)\n",
    "\n",
    "        # determinant of logit function\n",
    "        tf_log_jacobian_determinant = tf.reduce_sum(\n",
    "            tf.reshape(\n",
    "                tf.log((1.0 / tf_x) + (1.0 / (1.0 - tf_x))),\n",
    "                (-1, DIM)\n",
    "            ),\n",
    "            axis=-1\n",
    "        )\n",
    "        # determinant of normalization\n",
    "        tf_log_jacobian_determinant += np.log((1.0 - 2.0*alpha) / max_pixel_value) * DIM\n",
    "        \n",
    "        return tf_out, tf_log_jacobian_determinant\n",
    "\n",
    "    else:\n",
    "        tf_out = tf_in\n",
    "        # inverse logit\n",
    "        tf_out = tf.math.sigmoid(tf_out)\n",
    "        # inverse normalization\n",
    "        tf_out = (tf_out - alpha) * max_pixel_value / (1.0 - 2.0 * alpha)\n",
    "        \n",
    "        return tf_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copy pasted from samathas code for coupling\n",
    "def default_initializer(std=0.05):\n",
    "    return tf.random_normal_initializer(0., std)\n",
    "\n",
    "def flatten_sum(logps):\n",
    "    if len(logps.get_shape()) == 2:\n",
    "        return tf.reduce_sum(logps, [1])\n",
    "    elif len(logps.get_shape()) == 4:\n",
    "        return tf.reduce_sum(logps, [1, 2, 3])\n",
    "    else:\n",
    "        raise Exception()\n",
    "        \n",
    "def int_shape(x):\n",
    "    if str(x.get_shape()[0]) != '?':\n",
    "        return list(map(int, x.get_shape()))\n",
    "    return [-1]+list(map(int, x.get_shape()[1:]))\n",
    "\n",
    "def add_edge_padding(x, filter_size):\n",
    "    assert filter_size[0] % 2 == 1\n",
    "    if filter_size[0] == 1 and filter_size[1] == 1:\n",
    "        return x\n",
    "    a = (filter_size[0] - 1) // 2  # vertical padding size\n",
    "    b = (filter_size[1] - 1) // 2  # horizontal padding size\n",
    "    if True:\n",
    "        x = tf.pad(x, [[0, 0], [a, a], [b, b], [0, 0]])\n",
    "        name = \"_\".join([str(dim) for dim in [a, b, *int_shape(x)[1:3]]])\n",
    "        pads = tf.get_collection(name)\n",
    "        if not pads:\n",
    "            if False: #if hvd.rank() == 0:\n",
    "                print(\"Creating pad\", name)\n",
    "            pad = np.zeros([1] + int_shape(x)[1:3] + [1], dtype='float32')\n",
    "            pad[:, :a, :, 0] = 1.\n",
    "            pad[:, -a:, :, 0] = 1.\n",
    "            pad[:, :, :b, 0] = 1.\n",
    "            pad[:, :, -b:, 0] = 1.\n",
    "            pad = tf.convert_to_tensor(pad)\n",
    "            tf.add_to_collection(name, pad)\n",
    "        else:\n",
    "            pad = pads[0]\n",
    "        pad = tf.tile(pad, [tf.shape(x)[0], 1, 1, 1])\n",
    "        x = tf.concat([x, pad], axis=3)\n",
    "    else:\n",
    "        pad = tf.pad(tf.zeros_like(x[:, :, :, :1]) - 1,\n",
    "                     [[0, 0], [a, a], [b, b], [0, 0]]) + 1\n",
    "        x = tf.pad(x, [[0, 0], [a, a], [b, b], [0, 0]])\n",
    "        x = tf.concat([x, pad], axis=3)\n",
    "    return x\n",
    "\n",
    "def Z_conv2d(name, x, width, filter_size=[3, 3], stride=[1, 1], pad=\"SAME\", do_weightnorm=False, do_actnorm=True, context1d=None, skip=1, edge_bias=True):\n",
    "    with tf.variable_scope(name):\n",
    "        if edge_bias and pad == \"SAME\":\n",
    "            x = add_edge_padding(x, filter_size)\n",
    "            pad = 'VALID'\n",
    "\n",
    "        n_in = int(x.get_shape()[3])\n",
    "\n",
    "        stride_shape = [1] + stride + [1]\n",
    "        filter_shape = filter_size + [n_in, width]\n",
    "        w = tf.get_variable(\"W\", filter_shape, tf.float32,\n",
    "                            initializer=default_initializer())\n",
    "        if do_weightnorm:\n",
    "            w = tf.nn.l2_normalize(w, [0, 1, 2])\n",
    "        if skip == 1:\n",
    "            x = tf.nn.conv2d(x, w, stride_shape, pad, data_format='NHWC')\n",
    "        else:\n",
    "            assert stride[0] == 1 and stride[1] == 1\n",
    "            x = tf.nn.atrous_conv2d(x, w, skip, pad)\n",
    "        if do_actnorm:\n",
    "            x = actnorm(\"actnorm\", x)\n",
    "        else:\n",
    "            x += tf.get_variable(\"b\", [1, 1, 1, width],\n",
    "                                 initializer=tf.zeros_initializer())\n",
    "\n",
    "        if context1d != None:\n",
    "            x += tf.reshape(linear(\"context\", context1d,\n",
    "                                   width), [-1, 1, 1, width])\n",
    "    return x\n",
    "\n",
    "def Z_conv2d_zeros(name, x, width, filter_size=[3, 3], stride=[1, 1], pad=\"SAME\", logscale_factor=3, skip=1, edge_bias=True):\n",
    "    with tf.variable_scope(name):\n",
    "        if edge_bias and pad == \"SAME\":\n",
    "            x = add_edge_padding(x, filter_size)\n",
    "            pad = 'VALID'\n",
    "\n",
    "        n_in = int(x.get_shape()[3])\n",
    "        stride_shape = [1] + stride + [1]\n",
    "        filter_shape = filter_size + [n_in, width]\n",
    "        w = tf.get_variable(\"W\", filter_shape, tf.float32,\n",
    "                            initializer=tf.zeros_initializer())\n",
    "        if skip == 1:\n",
    "            x = tf.nn.conv2d(x, w, stride_shape, pad, data_format='NHWC')\n",
    "        else:\n",
    "            assert stride[0] == 1 and stride[1] == 1\n",
    "            x = tf.nn.atrous_conv2d(x, w, skip, pad)\n",
    "        x += tf.get_variable(\"b\", [1, 1, 1, width],\n",
    "                             initializer=tf.zeros_initializer())\n",
    "        x *= tf.exp(tf.get_variable(\"logs\",\n",
    "                                    [1, width], initializer=tf.zeros_initializer()) * logscale_factor)\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def coupling_layer(tf_in, WIDTH, name=\"\", reverse=False):\n",
    "    with tf.variable_scope(name+'coupling_layer', reuse=tf.AUTO_REUSE):\n",
    "        conv_features = [None]\n",
    "\n",
    "        def coupling_network(tf_layer, reuse=False):\n",
    "            # From GLOW:\n",
    "            def f(name, h, width, n_out=None):\n",
    "                n_out = n_out or int(h.get_shape()[3])\n",
    "                with tf.variable_scope(name, reuse=reuse):\n",
    "                    # NOTE: adding do_actnorm = False because... actnorm_layer would add something to inverse_ops and I don't wanna think about that.\n",
    "                    h = tf.nn.relu(Z_conv2d(\"l_1\", h, width, do_actnorm=False))\n",
    "                    h = tf.nn.relu(Z_conv2d(\"l_2\", h, width, filter_size=[1, 1], do_actnorm=False))\n",
    "                    conv_features[0] = h\n",
    "                    h = Z_conv2d_zeros(\"l_last\", h, n_out)\n",
    "                return h\n",
    "            h = f(\"f1\", tf_layer, WIDTH, n_z)\n",
    "            shift = h[:, :, :, 0::2]\n",
    "            scale = tf.nn.sigmoid(h[:, :, :, 1::2] + 2.)\n",
    "            return shift, scale\n",
    "        \n",
    "        shape = int_shape(tf_in)\n",
    "        n_z = shape[3]\n",
    "        assert n_z % 2 == 0, shape\n",
    "        z1 = tf_in[:, :, :, :n_z // 2]\n",
    "        z2 = tf_in[:, :, :, n_z // 2:]\n",
    "        shift, scale = coupling_network(z1, reuse=False)     \n",
    "        \n",
    "        \n",
    "        if not reverse: \n",
    "            z2 += shift\n",
    "            z2 *= scale\n",
    "            tf_log_jacobian_determinant = tf.reduce_sum(tf.log(scale), axis=[1,2,3])\n",
    "            z = tf.concat([z1, z2], 3)\n",
    "            return z, tf_log_jacobian_determinant, conv_features[0]\n",
    "    \n",
    "        else: ## Inverse operation\n",
    "            z2 /= scale\n",
    "            z2 -= shift\n",
    "\n",
    "            tf_in = tf.concat([z1, z2], 3)\n",
    "            return tf_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def invertible_1x1_conv(z, reverse=False):\n",
    "    # Copied wholesale from glow\n",
    "    with tf.variable_scope(\"invertable_1x1\", reuse=tf.AUTO_REUSE):\n",
    "        shape = int_shape(z)\n",
    "        w_shape = [shape[3], shape[3]]\n",
    "        \n",
    "        # Sample a random orthogonal matrix:\n",
    "        w_init = np.linalg.qr(np.random.randn(\n",
    "            *w_shape))[0].astype('float32')\n",
    "        w = tf.get_variable(\"W\", dtype=tf.float32, initializer=w_init)\n",
    "        \n",
    "        dlogdet = tf.cast(tf.log(abs(tf.matrix_determinant(\n",
    "            tf.cast(w, 'float64')))), 'float32') * shape[1]*shape[2]\n",
    "\n",
    "        if not reverse:\n",
    "            _w = tf.reshape(w, [1, 1] + w_shape)\n",
    "            z = tf.nn.conv2d(z, _w, [1, 1, 1, 1],\n",
    "                             'SAME', data_format='NHWC')\n",
    "            return z, dlogdet\n",
    "\n",
    "        else:\n",
    "            _w = tf.matrix_inverse(w)\n",
    "            _w = tf.reshape(_w, [1, 1]+w_shape)\n",
    "            z = tf.nn.conv2d(z, _w, [1, 1, 1, 1],\n",
    "                             'SAME', data_format='NHWC')\n",
    "            return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(z):\n",
    "    n_z = int_shape(z)[3]\n",
    "    assert n_z % 2 == 0\n",
    "    z_main  = z[:, :, :, :n_z // 2]\n",
    "    z_other = z[:, :, :, n_z // 2:]\n",
    "    return z_main, z_other\n",
    "\n",
    "def split_reverse(z1, z2):\n",
    "    return tf.concat([z1, z2], axis=-1)\n",
    "\n",
    "def get_vectorize_shape(z):\n",
    "    shape = int_shape(z)\n",
    "    dim = shape[1] * shape[2] * shape[3] \n",
    "    return dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
