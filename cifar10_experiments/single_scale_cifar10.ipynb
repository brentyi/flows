{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "try:\n",
    "    import nbimporter\n",
    "    from utils import helpful_functions as hf\n",
    "    from utils.layers import squeeze_layer, preprocess_layer, coupling_layer, invertible_1x1_conv, int_shape, glow_coupling_layer, actnorm_layer\n",
    "except:\n",
    "    print(\"pip install nbimporter\")\n",
    "    raise Exception(\"need to install\")\n",
    "\n",
    "THIRTYTWO = 32\n",
    "THREE = 1\n",
    "DIM = THIRTYTWO * THIRTYTWO * THREE\n",
    "WIDTH = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"samples100_lambda0to1over500steps_butfirst5000unlabeled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data, train_labels_o, test_data, test_labels_o = hf.get_mnist(orig=True)\n",
    "\n",
    "# 59,000 for training, 1000 for validation\n",
    "training_data, train_labels_o, val_data, val_labels_o   = hf.split_train_val(training_data, train_labels_o, validation_size=1000)\n",
    "\n",
    "# get 1000 for labed supervised training\n",
    "training_data, train_labels_o, labeled_data, labeled_labels_o = hf.split_train_val(training_data, train_labels_o, validation_size=1000)\n",
    "\n",
    "#hf.show_sample(training_data[0])\n",
    "\n",
    "val_labels = hf.make_one_hot(val_labels_o)\n",
    "labeled_labels = hf.make_one_hot(labeled_labels_o)\n",
    "training_labels = hf.make_one_hot(train_labels_o)\n",
    "test_labels = hf.make_one_hot(test_labels_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network!\n",
      "Building inverse!\n",
      "Building classifier!\n",
      "Building loss!\n",
      "Initializing act norm!\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Testing Simple Model (no multiscale)\n",
    "'''\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "ph_x         = tf.placeholder(tf.float32, [None, THIRTYTWO, THIRTYTWO, THREE], name=\"x\")\n",
    "ph_inverse_z = tf.placeholder(tf.float32, [None, DIM], name=\"z\")\n",
    "op_init_actnorm = []\n",
    "\n",
    "#DEPTH_PER_LEVEL = 2\n",
    "#LEVELS = 1\n",
    "DEPTH_PER_LEVEL = 16\n",
    "LEVELS = 3\n",
    "\n",
    "##### <flow network>\n",
    "print(\"Building network!\")\n",
    "tf_layer = ph_x\n",
    "tf_log_jacobian_determinant = 0\n",
    "tf_layer, log_det = preprocess_layer(tf_layer)\n",
    "tf_log_jacobian_determinant += log_det\n",
    "for j in range(LEVELS):\n",
    "    tf_layer = squeeze_layer(tf_layer)\n",
    "    with tf.variable_scope(\"module_\" + str(j)):\n",
    "        for i in range(DEPTH_PER_LEVEL):\n",
    "            with tf.variable_scope(\"layer_\" + str(i)):\n",
    "                tf_layer, log_det = glow_coupling_layer(tf_layer, op_init_actnorm)\n",
    "                tf_log_jacobian_determinant += log_det\n",
    "\n",
    "\n",
    "restore_shape = list(tf_layer.shape)\n",
    "restore_shape[0] = -1\n",
    "tf_z = tf.reshape(tf_layer, (-1, DIM))\n",
    "\n",
    "##### <inverse>\n",
    "print(\"Building inverse!\")\n",
    "tf_layer = ph_inverse_z\n",
    "tf_layer = tf.reshape(tf_layer, restore_shape)\n",
    "for j in reversed(range(LEVELS)):\n",
    "    with tf.variable_scope(\"module_\" + str(j)):\n",
    "        for i in reversed(range(DEPTH_PER_LEVEL)):\n",
    "            with tf.variable_scope(\"layer_\" + str(i)):\n",
    "                tf_layer = glow_coupling_layer(tf_layer, op_init_actnorm, reverse=True)\n",
    "        tf_layer = squeeze_layer(tf_layer, reverse=True)     \n",
    "\n",
    "tf_layer = preprocess_layer(tf_layer, reverse=True)        \n",
    "tf_inverse_x = tf_layer\n",
    "\n",
    "tf_sample_summary = tf.summary.image(\"samples\", tf_inverse_x)\n",
    "\n",
    "# <supervised training network>\n",
    "print(\"Building classifier!\")\n",
    "def res_block(tf_in, hidden_dim):\n",
    "    tf_layer = tf_in\n",
    "    tf_layer = tf.layers.dense(inputs=tf_layer, units=hidden_dim, activation=tf.nn.relu)\n",
    "    tf_layer = tf.layers.dense(inputs=tf_layer, units=hidden_dim)\n",
    "    return tf.nn.relu(tf_layer + tf_in)\n",
    "\n",
    "FC_UNITS      = 64\n",
    "NUM_RES_BLOCKS = 1\n",
    "ph_y          = tf.placeholder(tf.int32, [None,10], name=\"x\")\n",
    "\n",
    "tf_layer = tf.layers.dense(inputs=tf_z, units=FC_UNITS, activation=tf.nn.relu)\n",
    "with tf.name_scope(\"classifier\"):\n",
    "    for i in range(NUM_RES_BLOCKS):\n",
    "        tf_layer = res_block(tf_layer, FC_UNITS)\n",
    "    \n",
    "    tf_logits = tf.layers.dense(inputs=tf_layer, units=10)\n",
    "\n",
    "    tf_prob_y = tf.nn.softmax(tf_logits)\n",
    "    tf_classifier_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=tf_logits, labels=ph_y))\n",
    "\n",
    "    tf_true_labels = tf.argmax(input=ph_y, axis=1)\n",
    "    tf_predicted_labels = tf.argmax(input=tf_logits, axis=1)\n",
    "    tf_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf_true_labels, tf_predicted_labels), tf.float32))\n",
    "    \n",
    "    tf_classifier_validation_summary = tf.summary.merge([\n",
    "        tf.summary.scalar(\"classifier_accuracy_validation\", tf_accuracy)\n",
    "    ])\n",
    "    \n",
    "    tf_classifier_summary = tf.summary.merge([\n",
    "        tf.summary.scalar(\"classifier_accuracy\", tf_accuracy),\n",
    "        tf.summary.scalar(\"classifier_loss\", tf_classifier_loss)\n",
    "    ])\n",
    "\n",
    "##### <loss> & training!!\n",
    "print(\"Building loss!\")\n",
    "tf_global_step = tf.train.get_or_create_global_step()\n",
    "ph_learning_rate = tf.placeholder(tf.float32, shape=(), name=\"learning_rate\")\n",
    "ph_lambda = tf.placeholder(tf.float32, shape=(), name=\"loss_weight\")\n",
    "\n",
    "with tf.name_scope(\"flow_training\"):\n",
    "    tf_log_prob_z = - DIM * 0.5 * tf.log(2 * np.pi) - tf.reduce_sum(tf.square(tf_z) / 2.0, axis=-1)\n",
    "    tf_flow_loss = tf.reduce_mean(\n",
    "        (-tf_log_prob_z - tf_log_jacobian_determinant) / np.log(2) / DIM,\n",
    "        name=\"loss\"\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(ph_learning_rate)\n",
    "    tf_gradients, tf_variables = zip(*optimizer.compute_gradients(tf_flow_loss))\n",
    "    tf_gradients, tf_gradients_norm = tf.clip_by_global_norm(tf_gradients, 100)\n",
    "    op_flow_train = optimizer.apply_gradients(zip(tf_gradients, tf_variables), global_step=tf_global_step)\n",
    "\n",
    "    tf_flow_validation_summary = tf.summary.merge([\n",
    "        tf.summary.scalar(\"flow_loss_validation\", tf_flow_loss)\n",
    "    ])\n",
    "    tf_flow_loss_summary = tf.summary.scalar(\"flow_loss\", tf_flow_loss)\n",
    "    tf_flow_summary = tf.summary.merge([\n",
    "        tf_flow_loss_summary,\n",
    "        tf.summary.scalar(\"flow_gradients_norm\", tf_gradients_norm)\n",
    "    ])\n",
    "    \n",
    "with tf.name_scope(\"joint_training\"):\n",
    "    tf_joint_loss = tf_flow_loss * (1.0 - ph_lambda) + tf_classifier_loss * ph_lambda\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(ph_learning_rate)\n",
    "    tf_gradients, tf_variables = zip(*optimizer.compute_gradients(tf_joint_loss))\n",
    "    tf_gradients, tf_gradients_norm = tf.clip_by_global_norm(tf_gradients, 100)\n",
    "    op_joint_train = optimizer.apply_gradients(zip(tf_gradients, tf_variables), global_step=tf_global_step)\n",
    "\n",
    "    tf_joint_summary = tf.summary.merge([\n",
    "        tf_flow_loss_summary,\n",
    "        tf_classifier_summary,\n",
    "        tf.summary.scalar(\"joint_loss\", tf_joint_loss),\n",
    "        tf.summary.scalar(\"joint_gradients_norm\", tf_gradients_norm)\n",
    "    ])\n",
    "\n",
    "tf_validation_summary = tf.summary.merge([\n",
    "    tf_classifier_validation_summary,\n",
    "    tf_flow_validation_summary\n",
    "])\n",
    "\n",
    "\n",
    "op_init = tf.initializers.global_variables()\n",
    "sess.run(op_init)\n",
    "\n",
    "summary_writer = tf.summary.FileWriter('./log/{}'.format(EXPERIMENT_NAME), sess.graph)\n",
    "\n",
    "###### <running init>\n",
    "print(\"Initializing act norm!\")\n",
    "noise = np.random.uniform(0., 1, size=training_data[:100].shape)\n",
    "for op in op_init_actnorm:\n",
    "    sess.run(op, feed_dict={ph_x: training_data[:100] + noise})\n",
    "loss_history = []\n",
    "\n",
    "print(\"Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### <running forwards and backwards pass>\n",
    "original = training_data[3:4,:,:,:]\n",
    "\n",
    "z_state = sess.run(tf_z, feed_dict={ ph_x: original })\n",
    "z1 = z_state\n",
    "inv_img = sess.run(tf_inverse_x, feed_dict={ph_inverse_z: z_state})\n",
    "\n",
    "###### <verification>\n",
    "#hf.show_sample(original[0])\n",
    "#hf.show_sample(inv_img[0])\n",
    "assert hf.is_same(original[0], inv_img[0]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAABTCAYAAAB59HnxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrLjZK1N7V1UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 400x100 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_some_samples():\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=4)\n",
    "    fig.set_figheight(1)\n",
    "    fig.set_figwidth(4)\n",
    "    i = 120\n",
    "    for col in ax:\n",
    "        latent = np.random.normal(size=[DIM], scale=1)\n",
    "        #print(latent.shape)\n",
    "        sample = sess.run(tf_inverse_x, feed_dict={ph_inverse_z: [latent]})\n",
    "        #print(sample.shape)\n",
    "        hf.show_sample(sample[0], show=False, plot=col)\n",
    "    plt.show()\n",
    "\n",
    "show_some_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "VISUALIZATION_LATENTS = np.random.normal(size=[5, DIM], scale=1)\n",
    "np.random.seed()\n",
    "\n",
    "def train(training_data=None,\n",
    "          validation_data=None,\n",
    "          validation_labels=None,\n",
    "          labeled_training=None,\n",
    "          labeled_labels=None,          \n",
    "          iterations=20000,\n",
    "          flow_batch_size=64,\n",
    "          flow_per_validation=1,\n",
    "          loss_history=None,\n",
    "          learning_rate=0.001,\n",
    "          lambda_weight=0,\n",
    "          supervised_batch_size=0\n",
    "         ):\n",
    "\n",
    "        \n",
    "    if not callable(learning_rate):\n",
    "        learning_rate_val = learning_rate\n",
    "        learning_rate = lambda _: learning_rate_val\n",
    "    \n",
    "    if not callable(lambda_weight):\n",
    "        learning_rate_val = lambda_weight\n",
    "        lambda_weight = lambda _: learning_rate_val\n",
    "    \n",
    "    # training_data = training_data.copy() * 1.0\n",
    "    validation_data = validation_data * 1.0\n",
    "    validation_data += np.random.uniform(low=0., high=1.0, size=validation_data.shape)\n",
    "\n",
    "    if loss_history == None:\n",
    "        loss_history = []\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=3)\n",
    "\n",
    "    step = 0\n",
    "    training_loss, validation_loss = 0.0, 0.0\n",
    "    supervised_loss, supervised_validation  = 0.0, 0.0\n",
    "    best_validation = 10000\n",
    "\n",
    "    start_time = time.time()\n",
    "    un_sup_idx = 0\n",
    "    sup_idx = 0\n",
    "    \n",
    "    if training_data is not None:\n",
    "        train_count = len(training_data)\n",
    "        total_batches = int(train_count / flow_batch_size)\n",
    "    if labeled_training is not None:\n",
    "        labeled_train_count = len(labeled_training)\n",
    "        labeled_total_batches = int(labeled_train_count / supervised_batch_size)\n",
    "    \n",
    "    validation_step = 20\n",
    "    save_step = 400\n",
    "\n",
    "    for step in range(iterations):\n",
    "\n",
    "        if step % validation_step == 0:\n",
    "            # Add samples to tensorboard\n",
    "            summary, global_step = sess.run([tf_sample_summary, tf_global_step], feed_dict={ph_inverse_z: VISUALIZATION_LATENTS})\n",
    "            summary_writer.add_summary(summary, global_step)\n",
    "            \n",
    "            validation_loss, summary, global_step = sess.run(\n",
    "                [tf_flow_loss, tf_validation_summary, tf_global_step],\n",
    "                feed_dict={\n",
    "                    ph_x: validation_data + np.random.uniform(low=0.0, high=1.0, size=validation_data.shape),\n",
    "                    ph_y: validation_labels\n",
    "                }\n",
    "            )\n",
    "            summary_writer.add_summary(summary, global_step)\n",
    "\n",
    "        if step % save_step == 0:\n",
    "            print(\"saving\")\n",
    "            save_path = saver.save(sess, \"ckpts/model_{}_{}.ckpt\".format(EXPERIMENT_NAME, step))\n",
    "                    \n",
    "                    \n",
    "        for fpv in range(flow_per_validation):\n",
    "            if not flow_batch_size == 0:\n",
    "\n",
    "                if un_sup_idx == 0:\n",
    "                    np.random.shuffle(training_data)\n",
    "                minibatch = np.array(training_data[ (un_sup_idx*flow_batch_size) \n",
    "                                                  : ((un_sup_idx+1) * flow_batch_size) , :])\n",
    "                un_sup_idx += 1\n",
    "                un_sup_idx = un_sup_idx % total_batches\n",
    "\n",
    "                training_loss, _, summary, global_step = sess.run(\n",
    "                    [tf_flow_loss, op_flow_train, tf_flow_summary, tf_global_step],\n",
    "                    feed_dict={\n",
    "                        ph_x: minibatch + np.random.uniform(low=0.0, high=1.0, size=minibatch.shape),\n",
    "                        ph_learning_rate: learning_rate(step)\n",
    "                    }\n",
    "                )\n",
    "                summary_writer.add_summary(summary, global_step)\n",
    "\n",
    "        if not supervised_batch_size == 0:\n",
    "\n",
    "            # train supervised objective\n",
    "            if sup_idx == 0:\n",
    "                idxs = np.arange(0, labeled_train_count)\n",
    "                np.random.shuffle(idxs)\n",
    "                labeled_training  = labeled_training[idxs]\n",
    "                labeled_labels    = labeled_labels[idxs]\n",
    "\n",
    "            minibatch = np.array(labeled_training[(sup_idx*supervised_batch_size) \n",
    "                                                  : ((sup_idx+1) * supervised_batch_size) , :])\n",
    "            minibatch_labels = np.array(labeled_labels[(sup_idx*supervised_batch_size) \n",
    "                                                       : ((sup_idx+1) * supervised_batch_size)])\n",
    "            \n",
    "            sup_idx += 1\n",
    "            sup_idx = sup_idx % labeled_total_batches\n",
    "            \n",
    "            supervised_loss, _, summary, global_step = sess.run(\n",
    "                [tf_joint_loss, op_joint_train, tf_joint_summary, tf_global_step],\n",
    "                feed_dict={\n",
    "                    ph_x: minibatch + np.random.uniform(low=0.0, high=1.0, size=minibatch.shape),\n",
    "                    ph_y: minibatch_labels,\n",
    "                    ph_learning_rate: learning_rate(step),\n",
    "                    ph_lambda: lambda_weight(step)\n",
    "                }\n",
    "            )\n",
    "            summary_writer.add_summary(summary, global_step)\n",
    "            \n",
    "\n",
    "        if step % 20 == 0:\n",
    "            print(\"\\tSteps =\", step, \"\\tTraining loss =\", training_loss, \"\\tSupervised loss =\", supervised_loss)\n",
    "\n",
    "        loss_history.append((training_loss, validation_loss, supervised_loss, supervised_validation))\n",
    "        validation_loss = None\n",
    "        supervised_validation = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load a checkpoint\n",
    "# # restore to previous model\n",
    "if False:\n",
    "    print(\"start loading\")\n",
    "    \n",
    "    checkpoint_dir = \"trained_ckpts_mnist_16_depth\"\n",
    "    model_number = 199\n",
    "    #print(variables_can_be_restored)\n",
    "    variables_can_be_restored = tf.train.list_variables(checkpoint_dir)\n",
    "    \n",
    "    tf.train.load_variable('/home/user/logdir/checkpoint', 'variable_name')\n",
    "    \n",
    "    \n",
    "    temp_saver = tf.train.Saver(variables_can_be_restored)\n",
    "    ckpt_state = tf.train.get_checkpoint_state(checkpoint_dir, \"model_\" + str(model_number) +  \".ckpt\")\n",
    "    print('Loading checkpoint %s' % ckpt_state.model_checkpoint_path)\n",
    "    temp_saver.restore(sess, ckpt_state.model_checkpoint_path)\n",
    "\n",
    "    \n",
    "    print(\"done loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Only Unsupervised\n",
    "def lr_with_warmup(step):\n",
    "    warmup_steps = 250\n",
    "    learning_rate = 1e-3\n",
    "    return min(learning_rate * step / warmup_steps, learning_rate)\n",
    "\n",
    "train(\n",
    "    training_data,\n",
    "    val_data,\n",
    "    val_labels,\n",
    "    iterations=500,\n",
    "    flow_batch_size=64*2,\n",
    "    loss_history=loss_history,\n",
    "    learning_rate=lr_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n",
      "\tSteps = 0 \tTraining loss = 0.0 \tSupervised loss = 8.537121\n",
      "\tSteps = 20 \tTraining loss = 0.0 \tSupervised loss = 4.8443227\n",
      "\tSteps = 40 \tTraining loss = 0.0 \tSupervised loss = 4.3209615\n",
      "\tSteps = 60 \tTraining loss = 0.0 \tSupervised loss = 3.9000802\n",
      "\tSteps = 80 \tTraining loss = 0.0 \tSupervised loss = 3.3920403\n",
      "\tSteps = 100 \tTraining loss = 0.0 \tSupervised loss = 2.8421814\n",
      "\tSteps = 120 \tTraining loss = 0.0 \tSupervised loss = 2.146257\n",
      "\tSteps = 140 \tTraining loss = 0.0 \tSupervised loss = 1.9767644\n",
      "\tSteps = 160 \tTraining loss = 0.0 \tSupervised loss = 1.6928744\n",
      "\tSteps = 180 \tTraining loss = 0.0 \tSupervised loss = 1.3831701\n",
      "\tSteps = 200 \tTraining loss = 0.0 \tSupervised loss = 1.1517577\n",
      "\tSteps = 220 \tTraining loss = 0.0 \tSupervised loss = 1.015177\n",
      "\tSteps = 240 \tTraining loss = 0.0 \tSupervised loss = 0.94834316\n",
      "\tSteps = 260 \tTraining loss = 0.0 \tSupervised loss = 0.83845997\n",
      "\tSteps = 280 \tTraining loss = 0.0 \tSupervised loss = 0.7334696\n",
      "\tSteps = 300 \tTraining loss = 0.0 \tSupervised loss = 0.65404934\n",
      "\tSteps = 320 \tTraining loss = 0.0 \tSupervised loss = 0.5786562\n",
      "\tSteps = 340 \tTraining loss = 0.0 \tSupervised loss = 0.4982721\n",
      "\tSteps = 360 \tTraining loss = 0.0 \tSupervised loss = 0.4399027\n",
      "\tSteps = 380 \tTraining loss = 0.0 \tSupervised loss = 0.36718163\n",
      "saving\n",
      "\tSteps = 400 \tTraining loss = 0.0 \tSupervised loss = 0.28543717\n",
      "\tSteps = 420 \tTraining loss = 0.0 \tSupervised loss = 0.23028222\n",
      "\tSteps = 440 \tTraining loss = 0.0 \tSupervised loss = 0.16895548\n",
      "\tSteps = 460 \tTraining loss = 0.0 \tSupervised loss = 0.11167444\n",
      "\tSteps = 480 \tTraining loss = 0.0 \tSupervised loss = 0.054967362\n",
      "\tSteps = 500 \tTraining loss = 0.0 \tSupervised loss = 7.119589e-05\n",
      "\tSteps = 520 \tTraining loss = 0.0 \tSupervised loss = 1.3930739e-05\n",
      "\tSteps = 540 \tTraining loss = 0.0 \tSupervised loss = 8.2887215e-07\n",
      "\tSteps = 560 \tTraining loss = 0.0 \tSupervised loss = 3.6507805e-07\n",
      "\tSteps = 580 \tTraining loss = 0.0 \tSupervised loss = 9.3690335e-07\n",
      "\tSteps = 600 \tTraining loss = 0.0 \tSupervised loss = 4.079178e-07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-f8271d7270d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m       \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr_with_warmup\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m       \u001b[0mlambda_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlambda_with_warmup\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m       \u001b[0msupervised_batch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m      )\n",
      "\u001b[1;32m<ipython-input-56-532822a2dbc2>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(training_data, validation_data, validation_labels, labeled_training, labeled_labels, iterations, flow_batch_size, flow_per_validation, loss_history, learning_rate, lambda_weight, supervised_batch_size)\u001b[0m\n\u001b[0;32m    117\u001b[0m                     \u001b[0mph_y\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mminibatch_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                     \u001b[0mph_learning_rate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                     \u001b[0mph_lambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlambda_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m                 }\n\u001b[0;32m    121\u001b[0m             )\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Only Joint Training\n",
    "\n",
    "def lr_with_warmup(step):\n",
    "    warmup_steps = 1000\n",
    "    learning_rate = 1e-3\n",
    "    return min(learning_rate * step / warmup_steps, learning_rate)\n",
    "\n",
    "def lambda_with_warmup(step):\n",
    "    warmup_steps = 500\n",
    "    lambda_val = 1\n",
    "    return min(lambda_val * step / warmup_steps, lambda_val)\n",
    "\n",
    "\n",
    "train(training_data=None,\n",
    "      validation_data=val_data,\n",
    "      validation_labels=val_labels,\n",
    "      labeled_training=labeled_data[:100],\n",
    "      labeled_labels=labeled_labels[:100],\n",
    "      iterations=20000,\n",
    "      flow_batch_size=0,\n",
    "      loss_history=loss_history,\n",
    "      learning_rate=lr_with_warmup,\n",
    "      lambda_weight=lambda_with_warmup,\n",
    "      supervised_batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute NLL\n",
    "def nll(images, true_labels):\n",
    "    batch_size = 64\n",
    "    probs = []\n",
    "    total_batches = len(images) // batch_size\n",
    "    for qwe in range(total_batches):\n",
    "        minibatch = np.array(images[ (qwe*batch_size) : ((qwe+1) * batch_size) , :])\n",
    "        prob = sess.run( [tf_prob_y],\n",
    "                        feed_dict={\n",
    "                            ph_x: minibatch + np.random.uniform(low=0.0, high=1.0, size=minibatch.shape),\n",
    "                        })\n",
    "        probs += prob\n",
    "    \n",
    "    computed_num = total_batches * batch_size\n",
    "    nll_list = []\n",
    "    #print(probs)\n",
    "    prob_list = probs[0]\n",
    "    # print(prob_list)\n",
    "    for p, label in zip(prob_list, true_labels[:computed_num]):\n",
    "        #print(label)\n",
    "        #print(p)\n",
    "        nll_list.append(-np.log(p[label]))\n",
    "        \n",
    "    return np.mean(np.array(nll_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nll(test_data, test_labels_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index=20\n",
    "im_test = test_data[index:index+1]\n",
    "im_label = test_labels[index:index+1]\n",
    "prob, true_labels, pred_labels = sess.run( [tf_prob_y, tf_true_labels, tf_predicted_labels],\n",
    "                feed_dict={\n",
    "                    ph_x: im_test + 0.5,# np.random.uniform(low=0.0, high=1.0, size=im_test.shape),\n",
    "                    ph_y: im_label\n",
    "                })\n",
    "print(prob, true_labels, pred_labels)\n",
    "hf.show_sample(im_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Semi Supervised Learning\n",
    "# Only classifer\n",
    "\n",
    "def lr_with_warmup(step):\n",
    "    warmup_steps = 1000\n",
    "    learning_rate = 1e-3\n",
    "    return min(learning_rate * step / warmup_steps, learning_rate)\n",
    "\n",
    "def lambda_weight_with_warmup(step):\n",
    "    warmup_steps = 10000\n",
    "    lambda_weight = 0.95\n",
    "    return min(lambda_weight * step / warmup_steps, lambda_weight)\n",
    "\n",
    "\n",
    "train(training_data=None,\n",
    "      validation_data=val_data,\n",
    "      validation_labels=val_labels,\n",
    "      labeled_training=labeled_data,\n",
    "      labeled_labels=labeled_labels,          \n",
    "      iterations=2000,\n",
    "      flow_batch_size=0,\n",
    "      flow_per_validation=0,\n",
    "      loss_history=loss_history,\n",
    "      learning_rate=lr_with_warmup,\n",
    "      lambda_weight=0.5,\n",
    "      supervised_batch_size=32\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Semi Supervised Learning\n",
    "#  classifer with all unlabled samples\n",
    "\n",
    "def lr_with_warmup(step):\n",
    "    warmup_steps = 1000\n",
    "    learning_rate = 1e-3\n",
    "    return min(learning_rate * step / warmup_steps, learning_rate)\n",
    "\n",
    "def lambda_weight_with_warmup(step):\n",
    "    warmup_steps = 10\n",
    "    lambda_weight = 0.95\n",
    "    return min(lambda_weight * step / warmup_steps, lambda_weight)\n",
    "\n",
    "\n",
    "train(training_data=training_data[0:50000],\n",
    "      validation_data=val_data,\n",
    "      validation_labels=val_labels,\n",
    "      labeled_training=labeled_data,\n",
    "      labeled_labels=labeled_labels,          \n",
    "      iterations=200000,\n",
    "      flow_batch_size=64,\n",
    "      flow_per_validation=1,\n",
    "      loss_history=loss_history,\n",
    "      learning_rate=lr_with_warmup,\n",
    "      lambda_weight=0.5,\n",
    "      supervised_batch_size=32\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save loss histroy\n",
    "with open('loss_histroy_single_scale_exp3.pkl', 'wb') as f:\n",
    "    pickle.dump(loss_history, f)\n",
    "loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sucsessful unit tests on layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
